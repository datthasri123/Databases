Verify the current network agent and Pod network configuration ?

NetworkPolicy only works if you're using a supported network agent. 
The default configuration doesn't use any NetworkPolicy. 
Run the following command to verify that you don't see any NetworkPolicy pods.

kubectl get all -n kube-system


==> Replace the network agent with the calico agent
You will need to install a network agent to work with NetworkPolicy, let's install the calico agent. To access it, clone the course Git repository

> git clone https://github.com/sandervanvugt/cka
> cd cka
> kubectl apply -f calico.yaml



To verify that the new network agent has been deployed successfully, run the following command. Repeat until all pods show as running:

> kubectl get pods -n kube-system

NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-b48d575fb-dn7h8   1/1     Running   0          43s
calico-node-8z7dw                         1/1     Running   0          43s
calico-node-p79gv                         1/1     Running   0          43s
coredns-787d4945fb-6qdw8                  1/1     Running   0          5m34s
coredns-787d4945fb-zvk2g                  1/1     Running   0          5m34s
etcd-controlplane                         1/1     Running   0          5m48s
kube-apiserver-controlplane               1/1     Running   0          5m48s
kube-controller-manager-controlplane      1/1     Running   0          5m50s
kube-proxy-flrqq                          1/1     Running   0          5m21s
kube-proxy-xcpdz                          1/1     Running   0          5m35s
kube-scheduler-controlplane               1/1     Running   0          5m48s
kubelet-csr-approver-68c8fcd559-96gk7     1/1     Running   0          5m20s
kubelet-csr-approver-68c8fcd559-vdt8g     1/1     Running   0          5m20s


==========================================================================================

To apply a label on node use the below command
> kubectl label node node01 color=blue


Create a new deployment named blue with the nginx image and 3 replicas
Name: blue
Replicas: 3
Image: nginx

> kubectl create deployment blue --image=nginx --replicas=3


Which nodes can the pods for the blue deployment be placed on?
> kubectl get nodes
> kubectl describe node node_name | grep -i Taints
> kubectl describe node controlplane | grep -i Taints
> kubectl describe node node01 | grep -i Taints


Set Node Affinity to the deployment to place the pods on node01 only.
  Name: blue
  Replicas: 3
  Image: nginx
  NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
  Key: color
  value: blue

 
> kubectl edit deployments.apps
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: color
                  operator: In
                  values:
                    - blue


> kubectl get pods -o wide // this will show you which pod is deployed on which node



Create a new deployment named red with the nginx image and 2 replicas, 
and ensure it gets placed on the controlplane node only.

Use the label key - 
node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

use > kubectl describe node node_name // to see all the labels that were created on that node

Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=controlplane
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=


apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        imagePullPolicy: Always
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists




Create a multi-container pod with 2 containers.

>>> Use the spec given below:
Name: yellow

Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

If the pod goes into the crashloopbackoff 
then add the command sleep 1000 in the lemon container.


apiVersion: v1
kind: Pod
metadata:
   name: yellow
spec:
  containers:
    - name: lemon
      image: busybox
      command:
       - sleep
       - "1000"
    - name: gold
      image: redis

// get the pods running in the namespace: elastic-stack
> kubectl get pods -n elastic-stack

// to verify the logs of the namespace
> kubectl -n elastic-stack logs kibana


The application outputs logs to the file /log/app.log. 
View the logs and try to identify the user having issues with Login.

Inspect the log file inside the pod. to do this execute the below command

> kubectl -n elastic-stack exec -it app -- cat /log/app.log

------------------------------------------------

Edit the pod in the elastic-stack namespace to add a sidecar container 
to send logs to Elastic Search. 
Mount the log volume to the sidecar container.
Only add a new container. Do not modify anything else. Use the spec provided below.

Note: State persistence concepts are discussed in detail later in this course. 
For now please make use of the below documentation link for updating the concerning pod.

https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/


 to capture any pod definition in to yaml file utilize the command 
 > kubectl get po pod-name -o yaml > tmp.txt

 
The solution manifest file as follows:
Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator

---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate

  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume


===========================================================


In a multi-container pod, each container is expected to run a process that stays alive as long as 
the POD’s lifecycle. For example in the multi-container pod that we talked about earlier that has 
a web application and logging agent, both the containers are expected to stay alive at all times. 
The process running in the log agent container is expected to stay alive as long as the web 
application is running. If any of them fail, the POD restarts.

But at times you may want to run a process that runs to completion in a container. 
For example, a process that pulls a code or binary from a repository that will be 
used by the main web application. That is a task that will be run only one time
 when the pod is first created. Or a process that waits for an external service 
 or database to be up before the actual application starts. 
 That’s where initContainers comes in.

An initContainer is configured in a pod-like all other containers, except that it is specified 
inside a initContainers section, like this:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: \['sh', '-c', 'echo The app is running! && sleep 3600'\]
  initContainers:
  - name: init-myservice
    image: busybox
    command: \['sh', '-c', 'git clone &nbsp;;'\]





When a POD is first created the initContainer is run, and the process in the initContainer
must run to a completion before the real container hosting the application starts.

You can configure multiple such initContainers as well, like how we did for multi-pod containers. 
In that case, each init container is run one at a time in sequential order.

If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the 
Init Container succeeds.


apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: \['sh', '-c', 'echo The app is running! && sleep 3600'\]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;'\]
  - name: init-mydb
    image: busybox:1.28
    command: \['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;'\]



>>> add the initContainer like below

  initContainers:
  - image: busybox
    name: red-initContainers
    command:
    - "sleep"
    - "20"




Update the pod red to use an initContainer 
that uses the busybox image and sleeps for 20 seconds

Delete and re-create the pod if necessary. But make sure no other configurations change.

Pod: red
initContainer Configured Correctly

apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"


=====================================================================

// for making HTTP check

readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10

  periodSeconds: 5

  failureThresold: 8



// for database

readinessProbe:
  tcpSocket:
    port: 8080


// to execute any scripts

readinessProbe:
  exec:
    command: 
    - cat
    - /app/is_ready

==================================================================

// similar to this we have liveliness probe as well

livenessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThresold: 8



// for database

livenessProbe:
  tcpSocket:
    port: 8080


// to execute any scripts

livenessProbe:
  exec:
    command: 
    - cat
    - /app/is_ready



controlplane ~ ➜  kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
simple-webapp-1   1/1     Running   0          9m16s

controlplane ~ ➜  kubectl get service
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          16m
webapp-service   NodePort    10.100.77.95   <none>        8080:30080/TCP   9m23s


script to make http requests


for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done


================================================================


Update the newly created pod 'simple-webapp-2' with a readinessProbe using the given spec

Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Readiness Probe: httpGet
Http Probe: /ready
Http Port: 8080

// to replace the existing pod you need to use the below command
> kubectl replace -f simple-webapp-2.yaml --force

// container in the pod is restarted

    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /ready
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

================================================================

Update both the pods with a livenessProbe using the given spec
Delete and recreate the PODs.

Pod Name: simple-webapp-1
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Period Seconds: 1


    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /ready
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /live
        port: 8080
        scheme: HTTP
      periodSeconds: 1


// both readiness probe and liveness probe

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always 
    name: simple-webapp
    ports:
    - containerPort: 8080 
      protocol: TCP
    readinessProbe: 
      failureThreshold: 3
      httpGet: 
        path: /ready
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /live
        port: 8080
        scheme: HTTP
      periodSeconds: 1
                        


// running the docker container in the detached mode

docker run -d kodekloud/event-simulator

//to see the logs of any docker

docker logs -f ecf

// to view logs in kubernetes, specific to pod only
kubectl logs -f <pod-name>

//to see the live stats on the pod, use the below

minikube addons enable metrics-server

1. other than minikube
2. you need to download required programs from git hub
3. kubectl create -f deploy/1.8+/ 


To see the metrics

kubectl top node
kubectl top pod



// to get the pods related to specific labels use the below command
> kubectl get pods --selector app=APP1


// select the container with required labels
> kubectl get pods --selector env=dev

// How many objects are in the prod environment 
// including PODs, ReplicaSets and any other objects?

> kubectl get all --selector env=prod


// Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
> kubectl get pods --selector env=prod,bu=finance,tier=frontend

// A ReplicaSet definition file is given replicaset-definition-1.yaml. 
// Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
// Once you fix the issue, create the replicaset from the definition file.

ReplicaSet: replicaset-1
Replicas: 2

//Rollout command
> kubectl rollout status deployment/my-app-deployment
> kubectl rollout history deployment/my-app-deployment

// deployment strategies

> Recreate (destory all and create all)
> Rolling update (kill one and create one)
  // once you make the changes to deployment file, go ahead and deploy it
  > kubectl apply -f deployment-definition.yml

> kubectl set image deployment/myapp-deployment \
                    nginx-container=nginx:1.9.1

// then new revision is created for every new deployment
// to see how the deployment has happend, then go with below command

> kubectl describe deployment myapp-deployment


========================================================================================

Here are some handy examples related to updating a Kubernetes Deployment:

Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:

> kubectl create deployment nginx --image=nginx:1.16
deployment.apps/nginx created

> kubectl rollout status deployment nginx  // to see the building status while deploying
Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
deployment "nginx" successfully rolled out

>


> kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1     <​none​>    

>
Using the – -revision flag:

Here revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the 
--revision flag:


> kubectl rollout history deployment nginx --revision=1

deployment.apps/nginx with revision #1
Pod Template:
Labels:       app=nginx
pod-template-hash=78449c65d4
Containers:
nginx:
Image:      nginx:1.16
Port:       <​none​> 
Host Port:  <​none​> 
Environment: <​none​> 
Mounts:      <​none​> 
Volumes:     


>
Using the – -record flag:

You would have noticed that the “change-cause” field is empty in the rollout history output. 
We can use the – -record flag to save the command used to create/update a deployment against the revision number.


> kubectl set image deployment nginx nginx=nginx:1.17 --recordFlag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

>

> kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <​none​> 
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true

>
You can now see that the change-cause is recorded for revision 2 of this deployment.

Let's make some more changes. 
In the example below, we are editing the deployment and changing the image from nginx:1.17 to nginx:latest 
while making use of the –record flag.

  

> kubectl edit deployments.apps nginx --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx edited

> kubectl rollout history deployment nginx
deployment.apps/nginx 

REVISION  CHANGE-CAUSE
1         <​none​> 
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true
3         kubectl edit deployments.apps nginx --record=true



> kubectl rollout history deployment nginx --revision=3
deployment.apps/nginx with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=787f54657b
  Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
  Containers:
   nginx:
    Image:      nginx
    Port:       <​none​> 
    Host Port:  <​none​> 
    Environment:        <​none​> 
    Mounts:     <​none​> 
  Volumes:      

  >
Undo a change:

Let's now rollback to the previous revision:

> kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         
3         kubectl edit deployments.apps nginx --record=true
4         kubectl set image deployment nginx nginx=nginx:1.17 --record=true



> kubectl rollout history deployment nginx --revision=3
deployment.apps/nginx with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=787f54657b
  Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
  Containers:
   nginx:
    Image:      nginx:latest
    Port:       
    Host Port:  
    Environment:        
    Mounts:     
  Volumes:      

> kubectl describe deployments. nginx | grep -i image:
    Image:        nginx:1.17

>
With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.


> kubectl rollout history deployment nginx --revision=1
deployment.apps/nginx with revision #1
Pod Template:
  Labels:       app=nginx
        pod-template-hash=78449c65d4
  Containers:
   nginx:
    Image:      nginx:1.16
    Port:         <​none​> 
    Host Port:     <​none​> 
    Environment:       <​none​> 
    Mounts:      <​none​> 
  Volumes:      

> kubectl rollout undo deployment nginx --to-revision=1
deployment.apps/nginx rolled back
To rollback to specific revision we will use the --to-revision flag.

With --to-revision=1, it will be rolled back with the first image we used to create a deployment as we can see in the rollout history output.

> kubectl describe deployments. nginx | grep -i image:
Image: nginx:1.16

// to edit a deployment
> kubectl edit deployment.apps/frontend     // on coming back from file, changes with respect to commands will be implemented

// blue-green strategy means both the deployment old ones and new ones will be running in parallel
// once all the tests were completed then we will move to new ones all at once

// canary updates means first we will route some part of the traffic to new deployments, and do all the tests. If the tests are successful then we
// do the rolling udpates to come to new pod

//The deployment called frontend app is exposed on the NodePort via a service. Identify the name of this service.

> kubectl describe service frontend-service 
Name:                     frontend-service
Namespace:                default
Labels:                   app=myapp
Annotations:              <none>
Selector:                 app=frontend
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.103.199.86
IPs:                      10.103.199.86
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30080/TCP
Endpoints:                10.244.0.4:8080,10.244.0.5:8080,10.244.0.6:8080 + 2 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


// to reduce the number of replicas of the deployment use this command
> kubectl scale deployment --replicas=1 frontend-v2
> kubectl scale deployment frontend --replicas=0
> kubectl scale deployment frontend-v2 --replicas=5


kubectl create -f /root/replicaset-definition-1.yaml
The ReplicaSet "replicaset-1" is invalid: 
spec.template.metadata.labels: Invalid value: 
map[string]string{"tier":"nginx"}: 
`selector` does not match template `labels`




Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.
Use the specification given on the below.
Job Name: throw-dice-job
Image Name: kodekloud/throw-dice

kubectl create job throw-dice-job --image=kodekloud/throw-dice --dry-run=client -o yaml  > throw-dice-job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: throw-dice-job
spec:
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - image: kodekloud/throw-dice
        name: throw-dice-job
        resources: {}
      restartPolicy: Never
status: {}



apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never

===========================================


Update the job definition to run as many times as required to get 3 successful 6's.
Delete existing job and create a new one with the given spec. 
Monitor and wait for the job to succeed.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Job Succeeded: True

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  completions: 3
  backoffLimit: 25
  template:
    spec:
      containers:
      - name: kodekloud/throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never


// Let us now schedule that job to run at 21:30 hours every day.
// Create a CronJob for this.

CronJob Name: throw-dice-cron-job
Image Name: kodekloud/throw-dice
Schedule: 30 21 * * *

apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      backoffLimit: 25
      template:
        spec:
          containers:
          - name: kodekloud/throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never


apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never


controlplane ~ ➜  kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   15m

controlplane ~ ➜  kubectl get service -n default
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   15m

// That is a default service created by Kubernetes at launch


What is the targetPort configured on the kubernetes service?
Run the command: kubectl describe service and look at TargetPort.

controlplane ~ ➜  kubectl describe service kubernetes 
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.0.1
IPs:               10.43.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         192.6.151.8:6443
Session Affinity:  None
Events:            <none>


How many Endpoints are attached on the kubernetes service?
    // from the above
       Endpoints:         192.6.151.8:6443

to create a service

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort


//How many network policies do you see in the environment?

> kubectl get networkpolicies.networking.k8s.io

//Which pod is the Network Policy applied on?

controlplane ~ ➜  kubectl describe networkpolicies.networking.k8s.io payroll-policy 
Name:         payroll-policy
Namespace:    default
Created on:   2024-05-09 01:39:08 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress




Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
Use the spec given below. 
You might want to enable ingress traffic to the pod to test your rules in the UI.
Also, ensure that you allow egress traffic to DNS ports TCP and UDP (port 53) to enable DNS resolution from the internal pod.

Policy Name: internal-policy
Policy Type: Egress
Egress Allow: payroll
Payroll Port: 8080
Egress Allow: mysql
MySQL Port: 3306

controlplane ~ ➜  kubectl describe service db-service 
Name:              db-service
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          name=mysql
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.110.21.50
IPs:               10.110.21.50
Port:              <unset>  3306/TCP
TargetPort:        3306/TCP
Endpoints:         10.244.0.6:3306
Session Affinity:  None
Events:            <none>

controlplane ~ ➜  kubectl describe service payroll-service 
Name:                     payroll-service
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 name=payroll
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.105.103.24
IPs:                      10.105.103.24
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30083/TCP
Endpoints:                10.244.0.5:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  PodSelector:
    matchLabels:
      





apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP


Note: We have also allowed Egress traffic to TCP and UDP port. 
This has been added to ensure that the internal DNS resolution works from the internal pod.

The kube-dns service is exposed on port 53

root@controlplane:~> kubectl get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   18m
